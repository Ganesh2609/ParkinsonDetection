{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Model(\n",
      "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Wav2Vec2LayerNormConvLayer(\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_projection): Wav2Vec2FeatureProjection(\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "      (conv): ParametrizedConv1d(\n",
      "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "        (parametrizations): ModuleDict(\n",
      "          (weight): ParametrizationList(\n",
      "            (0): _WeightNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (padding): Wav2Vec2SamePadLayer()\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "        (attention): Wav2Vec2FlashAttention2(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Wav2Vec2FeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
       "=================================================================================================================================================\n",
       "Wav2Vec2Model (Wav2Vec2Model)                                     [32, 41216]          [32, 128, 512]       1,024                True\n",
       "├─Wav2Vec2FeatureEncoder (feature_extractor)                      [32, 41216]          [32, 512, 128]       --                   True\n",
       "│    └─ModuleList (conv_layers)                                   --                   --                   --                   True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (0)                        [32, 1, 41216]       [32, 512, 8242]      6,656                True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (1)                        [32, 512, 8242]      [32, 512, 4120]      787,968              True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (2)                        [32, 512, 4120]      [32, 512, 2059]      787,968              True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (3)                        [32, 512, 2059]      [32, 512, 1029]      787,968              True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (4)                        [32, 512, 1029]      [32, 512, 514]       787,968              True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (5)                        [32, 512, 514]       [32, 512, 257]       525,824              True\n",
       "│    │    └─Wav2Vec2LayerNormConvLayer (6)                        [32, 512, 257]       [32, 512, 128]       525,824              True\n",
       "├─Wav2Vec2FeatureProjection (feature_projection)                  [32, 128, 512]       [32, 128, 1024]      --                   True\n",
       "│    └─LayerNorm (layer_norm)                                     [32, 128, 512]       [32, 128, 512]       1,024                True\n",
       "│    └─Linear (projection)                                        [32, 128, 512]       [32, 128, 1024]      525,312              True\n",
       "│    └─Dropout (dropout)                                          [32, 128, 1024]      [32, 128, 1024]      --                   --\n",
       "├─Wav2Vec2EncoderStableLayerNorm (encoder)                        [32, 128, 1024]      [32, 128, 1024]      --                   True\n",
       "│    └─Wav2Vec2PositionalConvEmbedding (pos_conv_embed)           [32, 128, 1024]      [32, 128, 1024]      --                   True\n",
       "│    │    └─ParametrizedConv1d (conv)                             [32, 1024, 128]      [32, 1024, 129]      8,389,760            True\n",
       "│    │    └─Wav2Vec2SamePadLayer (padding)                        [32, 1024, 129]      [32, 1024, 128]      --                   --\n",
       "│    │    └─GELUActivation (activation)                           [32, 1024, 128]      [32, 1024, 128]      --                   --\n",
       "│    └─Dropout (dropout)                                          [32, 128, 1024]      [32, 128, 1024]      --                   --\n",
       "│    └─ModuleList (layers)                                        --                   --                   --                   True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (0)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (1)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (2)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (3)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (4)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (5)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (6)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (7)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (8)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (9)               [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (10)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (11)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (12)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (13)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (14)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (15)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (16)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (17)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (18)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (19)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (20)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (21)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (22)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    │    └─Wav2Vec2EncoderLayerStableLayerNorm (23)              [32, 128, 1024]      [32, 128, 1024]      12,596,224           True\n",
       "│    └─LayerNorm (layer_norm)                                     [32, 128, 1024]      [32, 128, 1024]      2,048                True\n",
       "=================================================================================================================================================\n",
       "Total params: 315,438,720\n",
       "Trainable params: 315,438,720\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 212.10\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 2.64\n",
       "Forward/backward pass size (MB): 6614.02\n",
       "Params size (MB): 614.10\n",
       "Estimated Total Size (MB): 7230.76\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "inp = torch.rand(size=(32, 41216), dtype=torch.float16).to(device=device)\n",
    "\n",
    "summary(model=model,\n",
    "        input_data=inp,\n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width=20,\n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip1 = model.feature_extractor(inp)\n",
    "ip1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 1024]), torch.Size([32, 128, 512]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip2 = model.feature_projection(ip1.permute(0, 2, 1))\n",
    "ip2[0].shape, ip2[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip3 = model.encoder(ip2[0])\n",
    "ip3.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        output_type=Wav2Vec2BaseModelOutput,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "        modality=\"audio\",\n",
      "        expected_output=_EXPECTED_OUTPUT_SHAPE,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_values: Optional[torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        mask_time_indices: Optional[torch.FloatTensor] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        extract_features = self.feature_extractor(input_values)\n",
      "        extract_features = extract_features.transpose(1, 2)\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            # compute reduced attention_mask corresponding to feature vectors\n",
      "            attention_mask = self._get_feature_vector_attention_mask(\n",
      "                extract_features.shape[1], attention_mask, add_adapter=False\n",
      "            )\n",
      "\n",
      "        hidden_states, extract_features = self.feature_projection(extract_features)\n",
      "        hidden_states = self._mask_hidden_states(\n",
      "            hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n",
      "        )\n",
      "\n",
      "        encoder_outputs = self.encoder(\n",
      "            hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = encoder_outputs[0]\n",
      "\n",
      "        if self.adapter is not None:\n",
      "            hidden_states = self.adapter(hidden_states)\n",
      "\n",
      "        if not return_dict:\n",
      "            return (hidden_states, extract_features) + encoder_outputs[1:]\n",
      "\n",
      "        return Wav2Vec2BaseModelOutput(\n",
      "            last_hidden_state=hidden_states,\n",
      "            extract_features=extract_features,\n",
      "            hidden_states=encoder_outputs.hidden_states,\n",
      "            attentions=encoder_outputs.attentions,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(model.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19028"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import Wav2Vec2PretrainDataset\n",
    "\n",
    "dataset = Wav2Vec2PretrainDataset(root='E:\\Amrita\\Subjects\\Sem 5\\BMSP paper work\\Dataset\\Italian health pretrain 1')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41216])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]['Input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_data_loaders\n",
    "\n",
    "train_loader = get_data_loaders(root1='E:\\Amrita\\Subjects\\Sem 5\\BMSP paper work\\Dataset\\Spanish healthy pretrain 1', root2='E:\\Amrita\\Subjects\\Sem 5\\BMSP paper work\\Dataset\\Italian health pretrain 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input': tensor([[-5.1446e-02, -1.3312e-02,  4.3040e-03,  ...,  1.3010e-01,\n",
       "           1.5021e-01,  1.4316e-01],\n",
       "         [ 3.1222e-02,  3.8302e-02,  1.6557e-02,  ..., -2.8636e-01,\n",
       "          -4.0722e-01, -4.2896e-01],\n",
       "         [-2.8291e-01, -3.1649e-01, -3.5919e-01,  ...,  2.3169e-01,\n",
       "           1.2125e-01,  2.1746e-01],\n",
       "         ...,\n",
       "         [-1.3131e-02, -7.1301e-02, -1.5411e-01,  ...,  2.1850e+00,\n",
       "           1.8551e+00,  1.9024e+00],\n",
       "         [-9.5647e-01, -1.0360e+00, -1.0356e+00,  ...,  1.0123e-01,\n",
       "           1.5509e-03, -1.0434e-01],\n",
       "         [-3.5962e+00, -3.8776e+00, -3.3064e+00,  ...,  1.1614e-02,\n",
       "          -6.3533e-03, -2.1652e-02]]),\n",
       " 'Mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
